PyTorch version: 2.9.0+cu128
Transformers version: 4.57.1
Using device: cuda
Loading model...
Gradient checkpointing enabled
Trainable parameters: 3,001,280 (1.99% of total)
Using stub implementation for pre-trained GNN (avoiding TorchDrug)
Loading dataset...
Using efficient pre-processed dataset...
Structural tokens available: True
Pre-computed embeddings found but disabled (may have wrong dimension)
Embeddings will be generated on-the-fly with hidden_dim=640
Loaded structural tokens for 11157 proteins
Loaded 11158 proteins from pre-processed dataset
Dataset split: 8926 training samples, 2232 validation samples
Mixed precision training enabled
Starting training for 10 epochs...
Effective batch size: 8

Epoch 1/10
Input IDs shape: torch.Size([2, 512])
Attention mask shape: torch.Size([2, 512])
N coordinates shape: torch.Size([2, 512, 3])
CA coordinates shape: torch.Size([2, 512, 3])
C coordinates shape: torch.Size([2, 512, 3])
Structural tokens available: True
Pre-computed embeddings available: False
Structural tokens shape: torch.Size([2, 512])
Actual sequence lengths in batch: [71, 428]
Max sequence length in batch: 428
‚ö†Ô∏è  WARNING: Found sequence of length 428, this may cause training to be very slow.
   Consider using --max_seq_len 200 or shorter for faster training during debugging.
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mesm2_lora_constraints_esm2_t30_150M_UR50D[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251031_053737-e7t31v3f/logs[0m
