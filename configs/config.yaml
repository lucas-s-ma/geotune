# Main configuration for GeoTune New

# Model configuration
model:
  model_name: "facebook/esm2_t6_8M_UR50D"  # Base ESM2 model (8M parameters)
  # Other options: facebook/esm2_t12_35M_UR50D, facebook/esm2_t30_150M_UR50D, facebook/esm2_t33_650M_UR50D
  constraint_weight: 5.0  # Weight for geometric constraint loss (increased from 0.1 to balance with other losses)
  dist_threshold: 15.0    # Distance threshold for constraints (Angstroms)

# LoRA configuration
lora:
  r: 8                    # LoRA attention dimension
  alpha: 16               # LoRA scaling factor
  dropout: 0.1            # LoRA dropout rate
  target_modules:         # Modules to apply LoRA to
    - "query"
    - "key"
    - "value"
    - "dense"
    - "intermediate.dense"
    - "output.dense"

# Training configuration
training:
  batch_size: 16           # Batch size - reduced for 16GB GPU
  gradient_accumulation_steps: 4  # Accumulate gradients (effective batch_size = 2*4 = 8)
  learning_rate: 1e-3     # Learning rate (used if primal_lr and dual_lr not specified)
  primal_lr: 1e-3         # Learning rate for primal task (ESM model + MLM head)
  dual_lr: 5e-1           # Learning rate for dual task (structure alignment loss module)
  num_epochs: 50          # Number of epochs
  warmup_steps: 100       # Warmup steps for scheduler
  max_seq_len: 512        # Maximum sequence length - reduced for memory
  seed: 1644                # Random seed
  output_dir: "outputs"   # Output directory
  checkpoint_frequency: 1 # Save checkpoint every N epochs (default: 2, set to 1 for testing)
  use_gradient_checkpointing: true  # Enable gradient checkpointing to save memory
  mixed_precision: true   # Use fp16 mixed precision training

# Data configuration
data:
  data_path: "data/processed"   # Path to processed data files
  num_workers: 2          # Number of data loading workers

# Data pipeline configuration
data_pipeline:
  embedding_dim: 320      # Dimension of GearNet embeddings (must match ESM2 hidden_size: 8M=320, 35M=480, 150M=640)
  foldseek_db_path: "data/foldseek_db" # Path to Foldseek database

# Evaluation configuration
evaluation:
  eval_data_path: "data/processed_test" # Path to evaluation data
  eval_batch_size: 8      # Batch size for evaluation

# Logging configuration
logging:
  use_wandb: true         # Whether to use Weights & Biases
  project_name: "geotune_online"  # W&B project name
  lambda_log_frequency: 1 # Log lambda distributions every N epochs (default: 1 = every epoch)
  
# Constraint configuration
constraints:
  distance_weight: 1.0    # Weight for distance constraints
  angle_weight: 0.5       # Weight for angle constraints
  neighborhood_weight: 0.5  # Weight for neighborhood constraints
  use_structure_alignment: true  # Enable structure alignment with on-the-fly embedding generation and caching
