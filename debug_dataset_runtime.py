#!/usr/bin/env python3
"""
Debug script to check token IDs generated by the Dataset at runtime.
This checks what __getitem__ actually returns during training.
"""
import sys
import torch
from pathlib import Path
import numpy as np

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from utils.data_utils import EfficientProteinDataset

def debug_dataset_runtime(data_path):
    """Check tokens generated by Dataset.__getitem__"""
    print("="*60)
    print("RUNTIME DATASET TOKEN VALIDATOR")
    print("="*60)
    print(f"\nLoading dataset from: {data_path}\n")

    # Create dataset instance (this is what training does)
    # Note: data_path should be the DIRECTORY, not the .pkl file
    # EfficientProteinDataset will look for processed_dataset.pkl inside this directory
    import os
    if data_path.endswith('.pkl'):
        # If user provided the .pkl file path, extract the directory
        data_path = os.path.dirname(data_path)

    dataset = EfficientProteinDataset(
        processed_data_path=data_path,  # Directory path, not file path
        max_seq_len=512,
        include_structural_tokens=False,  # Don't need structural tokens for this test
        load_embeddings=False
    )

    print(f"Dataset size: {len(dataset)} proteins\n")

    # Check first 100 samples
    all_tokens = []
    invalid_samples = []

    print("Checking first 100 samples...")
    for idx in range(min(100, len(dataset))):
        try:
            item = dataset[idx]

            # Get input_ids from the item
            input_ids = item['input_ids']  # This is a torch.Tensor

            # Convert to list
            if isinstance(input_ids, torch.Tensor):
                tokens = input_ids.tolist()
            else:
                tokens = list(input_ids)

            all_tokens.extend(tokens)

            # Check for invalid tokens
            # ESM2 valid: 0-23, 32
            invalid = [t for t in tokens if t > 32 or (t > 23 and t < 32)]
            if invalid:
                invalid_samples.append({
                    'idx': idx,
                    'protein_id': item.get('protein_id', 'unknown'),
                    'invalid_tokens': invalid[:10],
                    'sample_tokens': tokens[:30]
                })

        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            import traceback
            traceback.print_exc()

    # Statistics
    all_tokens = np.array(all_tokens)
    print(f"\n{'='*60}")
    print("TOKEN STATISTICS FROM __getitem__")
    print(f"{'='*60}")
    print(f"Total tokens checked: {len(all_tokens)}")
    print(f"Token range: {all_tokens.min()} - {all_tokens.max()}")
    print(f"Unique tokens: {sorted(np.unique(all_tokens).tolist())}")

    # Expected ESM2 tokens
    expected_tokens = set([0, 1, 2, 3] + list(range(4, 24)) + [32])
    actual_tokens = set(all_tokens.tolist())
    unexpected = actual_tokens - expected_tokens

    print(f"\n{'='*60}")
    print("VALIDATION")
    print(f"{'='*60}")

    if unexpected:
        print(f"❌ INVALID TOKENS FOUND: {sorted(unexpected)}")
        print(f"   Number of samples with invalid tokens: {len(invalid_samples)}")
        print(f"\n   This will cause CUDA device-side assert errors!")
    else:
        print(f"✅ All tokens are valid ESM2 tokens")

    # Token distribution
    print(f"\n{'='*60}")
    print("TOKEN DISTRIBUTION (Top 30)")
    print(f"{'='*60}")

    token_names = {
        0: '<cls>', 1: '<pad>', 2: '<eos>', 3: '<unk>',
        4: 'L', 5: 'A', 6: 'G', 7: 'V', 8: 'S', 9: 'E', 10: 'R', 11: 'T',
        12: 'I', 13: 'D', 14: 'P', 15: 'K', 16: 'Q', 17: 'N', 18: 'F', 19: 'Y',
        20: 'M', 21: 'H', 22: 'W', 23: 'C', 32: '<mask>'
    }

    unique, counts = np.unique(all_tokens, return_counts=True)
    sorted_indices = np.argsort(counts)[::-1]  # Sort by count descending

    print(f"{'Token ID':<10} {'Token':<8} {'Count':<12} {'%':<10}")
    print("-" * 50)
    for idx in sorted_indices[:30]:  # Top 30
        tok = unique[idx]
        cnt = counts[idx]
        tok_name = token_names.get(tok, 'INVALID')
        pct = (cnt / len(all_tokens)) * 100
        symbol = "✅" if tok in expected_tokens else "❌"
        print(f"{symbol} {tok:<10} {tok_name:<8} {cnt:<12} {pct:.2f}%")

    # Show invalid samples
    if invalid_samples:
        print(f"\n{'='*60}")
        print("INVALID SAMPLES (first 5)")
        print(f"{'='*60}")
        for sample in invalid_samples[:5]:
            print(f"\nSample {sample['idx']} ({sample['protein_id']}):")
            print(f"  Invalid tokens: {sample['invalid_tokens']}")
            print(f"  Sample tokens: {sample['sample_tokens']}")

    # Check for old mapping signature
    print(f"\n{'='*60}")
    print("CHECKING FOR OLD TOKEN MAPPING")
    print(f"{'='*60}")

    # Count amino acid tokens
    amino_acid_tokens = [t for t in all_tokens if 4 <= t <= 24]
    has_24 = 24 in actual_tokens
    has_4 = 4 in actual_tokens

    if has_24:
        print("❌ Found token ID 24!")
        print("   This suggests OLD mapping (A=5, R=6, ..., V=24)")
        print("   Check if data_utils.py was properly updated on cluster")
    elif has_4 and not has_24:
        print("✅ Using NEW mapping (L=4, A=5, ..., C=23)")
    else:
        print("⚠️  Unusual distribution")

    return len(invalid_samples) == 0


if __name__ == "__main__":
    data_path = sys.argv[1] if len(sys.argv) > 1 else "data/processed/processed_dataset.pkl"

    if not Path(data_path).exists():
        print(f"❌ Dataset not found: {data_path}")
        sys.exit(1)

    is_valid = debug_dataset_runtime(data_path)

    print(f"\n{'='*60}")
    if is_valid:
        print("✅ VALIDATION PASSED")
    else:
        print("❌ VALIDATION FAILED - Dataset generates invalid tokens")
        print("\nAction needed:")
        print("1. Verify data_utils.py on cluster has correct ESM2 mapping")
        print("2. Clear Python cache: find . -type d -name __pycache__ -exec rm -rf {} +")
        print("3. Restart training")
    print(f"{'='*60}\n")
